{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from trainingData.csv and trainingLabels.csv\n",
    "colnames_train = ['session_id', 'start_time', 'end_time', 'product_list']\n",
    "colnames_test = ['label']\n",
    "dateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S')\n",
    "df = pd.read_csv(\"./data/trainingData.csv\", names=colnames_train, parse_dates=['start_time', 'end_time'], header=None)\n",
    "df_test = pd.read_csv(\"./data/trainingLabels.csv\", names = colnames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Feature_Engineering\n",
    "df['session_duration'] = (df['end_time'] - df['start_time']).dt.total_seconds()\n",
    "df['product_count'] = df['product_list'].str.count('A')\n",
    "df['Average_duration_per_product'] = df['session_duration'] / df['product_count']\n",
    "df['day_of_week'] = df['end_time'].dt.dayofweek\n",
    "df['date'] = df['end_time'].dt.day\n",
    "df['month'] = df['end_time'].dt.month\n",
    "df['start_hour'] = df['start_time'].dt.hour\n",
    "df['end_hour'] = df['end_time'].dt.hour\n",
    "\n",
    "#Scaling the product_count variable\n",
    "sc = StandardScaler()\n",
    "prd_cnt = np.array(df['product_count'])\n",
    "prd_cnt = prd_cnt.reshape(-1, 1)\n",
    "df['product_count'] = sc.fit_transform(prd_cnt)\n",
    "df_dummy = df.iloc[:,7:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy encoding the categorical data such as day_of_week, date, month, start_hour, end_hour\n",
    "dummy = pd.get_dummies(df_dummy, columns=['day_of_week', 'date', 'month', 'start_hour', 'end_hour'])\n",
    "df = pd.concat([df, dummy], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the data for identifying the number of nodes at each level/hierarchy\n",
    "df['product_list'] = df.product_list.apply(lambda x: x.split('/'))\n",
    "df['product_list'] = df.product_list.apply(' '.join).str.replace('[^A-Za-z0-9,\\s]+', '').str.split(expand=False)\n",
    "df['new'] = df['product_list'].map(set)\n",
    "df['new'] = df['new'].apply(lambda x: ','.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating number of nodes at each level\n",
    "df['A_unique_count'] = df['new'].str.count('A')\n",
    "df['B_unique_count'] = df['new'].str.count('B')\n",
    "df['C_unique_count'] = df['new'].str.count('C')\n",
    "df['D_unique_count'] = df['new'].str.count('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the varibles which have been dummy-encoded\n",
    "df.drop(['day_of_week', 'date', 'month', 'start_hour', 'end_hour', 'new'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['session_id', 'start_time', 'end_time', 'product_list',\n",
       "       'session_duration', 'product_count', 'Average_duration_per_product',\n",
       "       'day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3',\n",
       "       'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'date_1', 'date_2',\n",
       "       'date_3', 'date_5', 'date_6', 'date_7', 'date_8', 'date_9', 'date_12',\n",
       "       'date_13', 'date_14', 'date_15', 'date_16', 'date_17', 'date_18',\n",
       "       'date_19', 'date_20', 'date_21', 'date_22', 'date_23', 'date_25',\n",
       "       'date_26', 'date_27', 'date_28', 'date_29', 'date_30', 'month_11',\n",
       "       'month_12', 'start_hour_0', 'start_hour_1', 'start_hour_2',\n",
       "       'start_hour_3', 'start_hour_4', 'start_hour_5', 'start_hour_6',\n",
       "       'start_hour_7', 'start_hour_8', 'start_hour_9', 'start_hour_10',\n",
       "       'start_hour_11', 'start_hour_12', 'start_hour_13', 'start_hour_14',\n",
       "       'start_hour_15', 'start_hour_16', 'start_hour_17', 'start_hour_18',\n",
       "       'start_hour_19', 'start_hour_20', 'start_hour_21', 'start_hour_22',\n",
       "       'start_hour_23', 'end_hour_0', 'end_hour_1', 'end_hour_2', 'end_hour_3',\n",
       "       'end_hour_4', 'end_hour_5', 'end_hour_6', 'end_hour_7', 'end_hour_8',\n",
       "       'end_hour_9', 'end_hour_10', 'end_hour_11', 'end_hour_12',\n",
       "       'end_hour_13', 'end_hour_14', 'end_hour_15', 'end_hour_16',\n",
       "       'end_hour_17', 'end_hour_18', 'end_hour_19', 'end_hour_20',\n",
       "       'end_hour_21', 'end_hour_22', 'end_hour_23', 'A_unique_count',\n",
       "       'B_unique_count', 'C_unique_count', 'D_unique_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index of columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy encoding products/hierarchical nodes using multilabel Binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_product = pd.DataFrame(mlb.fit_transform(df['product_list']),columns=mlb.classes_, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the products/hierarchical nodes that appear less than 3 times\n",
    "df_product.drop([col for col, val in df_product.sum().iteritems() if val < 3], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining the features extracted with the dummy encoded product/hierarchical data. Dropped redundant and unwanted data for model construction.\n",
    "df_product = pd.concat([df_product, df], axis = 1)\n",
    "df_product.drop(['product_list', 'session_id', 'start_time', 'end_time'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the target variable \"gender\"\n",
    "df_product['gender'] = df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting female and male class to 0 and 1 respectively\n",
    "labelencoder_y = LabelEncoder()\n",
    "df_product['gender'] = labelencoder_y.fit_transform(df_product['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "0    11703\n",
       "1     3297\n",
       "Name: day_of_week_1, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target varible - class proportion\n",
    "df_product.groupby(['gender'])['day_of_week_1'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under-Sampling\n",
    "df_female = len(df_product[df_product['gender'] == 1])\n",
    "female_indices = df_product[df_product['gender'] == 0].index\n",
    "random_indices = np.random.choice(female_indices, df_female, replace=False)\n",
    "male_indices = df_product[df_product['gender'] == 1].index\n",
    "under_sample_indices = np.concatenate([male_indices, random_indices])\n",
    "under_sample = df_product.loc[under_sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X - feature and y - target\n",
    "X_under = under_sample.loc[:,under_sample.columns != 'gender'].values\n",
    "y_under = under_sample.loc[:,under_sample.columns == 'gender'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5275, 2592)\n",
      "(5275, 1)\n",
      "(1319, 2592)\n",
      "(1319, 1)\n"
     ]
    }
   ],
   "source": [
    "#Train-test split - 80:20\n",
    "X_under_train, X_under_test, y_under_train, y_under_test = train_test_split(X_under,y_under,test_size = 0.2, random_state = 0)\n",
    "print(X_under_train.shape)\n",
    "print(y_under_train.shape)\n",
    "print(X_under_test.shape)\n",
    "print(y_under_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7765067002792359\n",
      "General accuracy: 0.7793783169067475\n",
      "confusion matrix:\n",
      " [[617  56]\n",
      " [235 411]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.92      0.81       673\n",
      "          1       0.88      0.64      0.74       646\n",
      "\n",
      "avg / total       0.80      0.78      0.77      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression model\n",
    "lr_under = LogisticRegression()\n",
    "lr_under.fit(X_under_train,y_under_train)\n",
    "y_under_pred = lr_under.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7681790789358678\n",
      "General accuracy: 0.7710386656557998\n",
      "confusion matrix:\n",
      " [[611  62]\n",
      " [240 406]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.91      0.80       673\n",
      "          1       0.87      0.63      0.73       646\n",
      "\n",
      "avg / total       0.79      0.77      0.77      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neural Network model\n",
    "MLP_Classifier = MLPClassifier(random_state=4)\n",
    "MLP_Classifier.fit(X_under_train,y_under_train)\n",
    "y_under_pred = MLP_Classifier.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7123583694837128\n",
      "General accuracy: 0.7164518574677786\n",
      "confusion matrix:\n",
      " [[614  59]\n",
      " [315 331]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.91      0.77       673\n",
      "          1       0.85      0.51      0.64       646\n",
      "\n",
      "avg / total       0.75      0.72      0.70      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear support vector classifier\n",
    "Linear_SVC_under = LinearSVC(random_state= 134)\n",
    "Linear_SVC_under.fit(X_under_train,y_under_train)\n",
    "y_under_pred = Linear_SVC_under.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7789195368457855\n",
      "General accuracy: 0.7824109173616376\n",
      "confusion matrix:\n",
      " [[639  34]\n",
      " [253 393]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.95      0.82       673\n",
      "          1       0.92      0.61      0.73       646\n",
      "\n",
      "avg / total       0.82      0.78      0.78      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gradient boost classifier\n",
    "GBM = GradientBoostingClassifier()\n",
    "GBM.fit(X_under_train,y_under_train)\n",
    "y_under_pred = GBM.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.776633207439541\n",
      "General accuracy: 0.778620166793025\n",
      "confusion matrix:\n",
      " [[588  85]\n",
      " [207 439]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.87      0.80       673\n",
      "          1       0.84      0.68      0.75       646\n",
      "\n",
      "avg / total       0.79      0.78      0.78      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random forest classifier\n",
    "Randomforest = RandomForestClassifier(class_weight= {0:0.22, 1:0.78}, random_state= 134)\n",
    "Randomforest.fit(X_under_train,y_under_train)\n",
    "y_under_pred = Randomforest.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7222788309818335\n",
      "General accuracy: 0.7217589082638363\n",
      "confusion matrix:\n",
      " [[469 204]\n",
      " [163 483]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.70      0.72       673\n",
      "          1       0.70      0.75      0.72       646\n",
      "\n",
      "avg / total       0.72      0.72      0.72      1319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "Decisiontree = DecisionTreeClassifier(class_weight= {0:0.22, 1:0.78})\n",
    "Decisiontree.fit(X_under_train,y_under_train)\n",
    "y_under_pred = Decisiontree.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the algorithm(computed as given in question): 0.7791990026635507\n",
      "General accuracy: 0.7824109173616376\n",
      "confusion matrix:\n",
      " [[630  43]\n",
      " [244 402]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.94      0.81       673\n",
      "          1       0.90      0.62      0.74       646\n",
      "\n",
      "avg / total       0.81      0.78      0.78      1319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\praveen.kumar\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Ensemble using Voting classifier of Decision tree, Random forest, linear support vector classifier, Neural network, Gradient Boost, Logistic regression\n",
    "ensemble = VotingClassifier(estimators = [('DT',Decisiontree), ('RF', Randomforest), ('SVC', Linear_SVC_under), ('GBM', GBM), ('Logistic_regression', lr_under), ('NN', MLP_Classifier)])\n",
    "ensemble.fit(X_under_train,y_under_train)\n",
    "y_under_pred = ensemble.predict(X_under_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_under_test,y_under_pred).ravel()\n",
    "score = ((tp/ (tp+fn)) + (tn/ (tn+fp)))/2\n",
    "print(\"Score of the algorithm(computed as given in question):\", score)\n",
    "print(\"General accuracy:\", accuracy_score(y_under_test, y_under_pred))\n",
    "print(\"confusion matrix:\\n\", confusion_matrix(y_under_test, y_under_pred))\n",
    "print(\"classification report:\\n\", classification_report(y_under_test, y_under_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
